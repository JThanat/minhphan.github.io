---
layout: post
comments: true
title:  "Deep Reinforcement Learning: Pong from Pixels"
excerpt: "I'll discuss the core ideas, pros and cons of policy gradients, a standard approach to the rapidly growing and exciting area of deep reinforcement learning. As a running example we'll learn to play ATARI 2600 Pong from raw pixels."
date:   2016-05-31 11:00:00
mathjax: true
---

<!-- 
<svg width="800" height="200">
	<rect width="800" height="200" style="fill:rgb(98,51,20)" />
	<rect width="20" height="50" x="20" y="100" style="fill:rgb(189,106,53)" />
	<rect width="20" height="50" x="760" y="30" style="fill:rgb(77,175,75)" />
	<rect width="10" height="10" x="400" y="60" style="fill:rgb(225,229,224)" />
</svg>
 -->

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{LDA.png}
    \caption{LDA graphical model}
    \label{fig.1}
\end{figure}

The purpose of this summary is to 1/ understand main components that build up LDA, the intuition how LDA works, 2/ the maths' derivation the author didn't include in the original paper and 3/the author's code implementation. These three are equally important in order to understand LDA theorically and practically. Let's start to look at the Dirichlet distribution. 
\subsection{Dirichlet Distribution} \label{Dirichlet}

To understand Dirichlet, let's start with a simpler case where $$K=2$$, the Dirichlet distribution becomes Beta distribution. Let $$X=[X_1,X_2]$$ is the random pmf of $$\text{Beta}(\alpha, \beta)$$. $$X \sim \text{Beta}(\alpha, \beta)$$ when:


\begin{align*}
    f(x;\alpha, \beta) &= \text{Beta}(\alpha, \beta) \\
    &=  \frac{1}{\text{B}(\alpha,\beta)} x^{\alpha -1}(1-x)^{\beta-1} \\
    &= \frac{ \Gamma\big(\alpha + \beta \big)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha -1}(1-x)^{\beta-1},\\
\end{align*}


You can see that the Bionomial distribution has a similar form $$p^k(1-p)^{n-k}$$:
\begin{quote}
    \enquote{the binomial distribution with parameters n and p is the discrete probability distribution of \textbf{the number of successes in a sequence} of n independent experiments, each asking a yes–no question} - Binomial distribution, Wikipedia
\end{quote}
With a fixed $$x_i=p_i$$, $$\alpha-1=k$$, $$\beta-1=n-k$$, we have $$\alpha+\beta=n-2$$ and $$x^{\alpha -1}(1-x)^{\beta-1} \sim p^k(1-p)^{n-k}$$. Now take an example of throwing a coin of two outcomes head $$\{H\}$$ or tail $$\{T\}$$ \enquote{in sequence or continously}. Let a \enquote{biased} coin with $$\alpha = 100, \beta = 200$$, i.e. $$X=[X_1,X_2]=[100 \text{ } H\text{s},200 \text{ } T\text{s}]$$. The probability of a 100 successful outcome $$H$$ in a 300 sequences of throwing coin with a random probability $$x$$ (of only a single success $$H$$) is  Beta($$\alpha,\beta$$). Now, consider its means:

\begin{equation} 
    \begin{align*}
        \mu = \text{E}[X_1] &= \int^1_0 xf(x;\alpha, \beta)dx \\
        &= \int^1_0 \frac{x^{\alpha -1}(1-x)^{\beta-1}}{ \text{B}(\alpha,\beta) }\\
        &= \frac{\alpha}{\alpha+\beta},
    \end{align*}
\end{equation}

Also the median $$\approx \frac{\alpha - \frac{1}{3}}{\alpha+\beta - \frac{2}{3}}$$, for $$\alpha,\beta \geq 1$$. With whatever the shape of this distribution is, its mean and median should be those numbers. The Beta distribution is simply defined with $$\alpha + \beta$$ experiments, with random variable of a single success outcome probability $$x$$, the expectation $$\text{E}[X_1=\alpha \text{ times of } x]=\frac{\alpha}{\alpha+\beta}$$. We can re-state our definition as: 
\begin{quote}
    The Beta distribution with parameter $$\alpha$$ and $$\beta$$ is the discrete probability distribution of $$\alpha$$ times successful outcomes with a random variable $$x$$ in a sequence of $$\alpha + \beta$$ independent experiments with a probability of $$\frac{\alpha}{\alpha+\beta}$$.
\end{quote}
Note that we put the statement of expected value $$\frac{\alpha}{\alpha+\beta}$$ in order to make a distinction to the bionomial distribution. Let's try this on R software:
\lstinputlisting[linerange={1-9}, caption=Beta Distribution Illustration,language=R]{Code/R.r}

% Consider a \enquote{biased} coin with initial value $$\alpha = 100, \beta = 200$$. We don't know the mean or $$\text{E}[X_1]$$ but we know $$\text{E}[X_1] = \frac{\alpha}{\alpha+\beta}= \frac{100}{100+200} = \frac{1}{3} \approx 0.33$$. If we continue to throw the coin 10 times, we get $$H$$ 2 times and $$T$$ 8 times. We adjust $$\text{E}[X_1] = \frac{\alpha}{\alpha+\beta}= \frac{100 + 2}{100+2+200+8} \approx 0.32 $$. If we continue to throw 1000 times, $$\text{E}[X_1]$$ will probabilty close to $$\frac{\alpha}{\alpha+\beta}= \frac{100}{100+200}$$.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=9cm]{mean.png}
%     \caption{Sampling from Beta Distribution $$(\alpha, \beta) = (100,200)$$}
%     \label{fig:curve}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=13cm]{beta-1.png}
    \caption{1/Beta Distribution $$0\leq x\leq 1, (\alpha, \beta) = (100,200)$$ and 2/Sampling from Beta distribution}
    \label{fig:curve}
\end{figure}

Now if your friend come to you and tell that he's writting a game for people to roll a 6-sides dice. He has observed that the set of sides  $$\{i\}^{K=6}_{i=1}$$, such that side $$i$$th appears $$\alpha_i$$ times in every sequence of $$\sum^6_{j=1}\alpha_i$$ with the probability of $$\alpha_i/\sum^6_{j=1}\alpha_i$$. This has been drawing a large number of people to play. Now he needs you to develop an algorithm to build such a \enquote{loaded} dice. How can you solve this? You have a set of outcomes of rolling a 6-sides dice $$X=[X_1,X_2,X_3,X_4,X_5,X_6]$$ and you know the prior mean value of each outcome $$\text{E}[X_i] = \alpha_i/\sum^6_{j=1}\alpha_i$$. You wish that if the dice only has 2 sides so you can use Beta distribution to solve your problem right away. Fortunately, this is where the Dirichlet distribution comes to the rescue. It's the expansion form of Beta distribution for $$K > 2$$.

\hfill \break
With a set of outcomes $$X=[X_1,X_2,\hdots,X_K]$$ be the random pmf, $$X \sim \text{Dir}(\alpha)$$ when:

\begin{equation} 
    \begin{align*}
        f(\boldm{x},\boldm{\alpha}) &=  \frac{1}{\text{B}(\boldm{\alpha})} \prod^K_{i=1} x_i^{\alpha_i - 1} \\
        &= \frac{ \Gamma\big(\sum^K_{i=1} \alpha_i \big)}{\prod^K_{i=1}\Gamma(\alpha_i)}\prod^K_{i=1} x_i^{\alpha_i - 1},\\
    \end{align*}
\end{equation}

where $$\Gamma(.)$$ is the gamma function, for positive integer $$n$$, $$\Gamma(n) = (n-1)!$$. We can also re-define the definition of Dirichlet distribution:
\begin{quote}
    The Dirichlet distribution with parameters $$\{\alpha_i\}^K_{i=1}$$ is the discrete probability distribution of $$\alpha_i$$ times successful outcomes with a random variable $$x_i$$ in a sequence of $$\sum^K_{j=1}\alpha_j$$ independent experiments with a probability of $$\alpha_i/\sum^K_{j=1}\alpha_j$$. 
\end{quote}

The Dirichlet distribution also looks something similar to multinomial distribution, but instead of in the form $$\prod^K_{i=1} p_i^{x_i - 1}$$ it takes $$\prod^K_{i=1} x_i^{\alpha_i - 1}$$. You can use R to sample from Dirichlet distribution:

\lstinputlisting[linerange={11-16}, caption=Sample from Dirichlet distribution with $$K=2$$,language=R]{Code/R.r}

From our new definition, if we have a corpus $$D$$ with $$K$$ topic, to find a distribution of $$\alpha_i$$ times of topic $$i$$th appears with a probability $$p_i$$ such that all topics' probabilities $$\sum^K_{i=1}p_i=1$$, we can use Dirichlet distribution to build up a LDA model.

\hfill \break
The last thing we might need to consider another aspect of the distribution; its variance:
\begin{equation*}
    \text{Var}[X] = \ddfrac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}.
\end{equation*}
Let's try some values of $$(\alpha, \beta)$$ on R to see how the shape changes:
\lstinputlisting[linerange={18-21}, caption=Beta Distribution Illustration,language=R]{Code/R.r}

\begin{figure}[h]
    \centering
    \includegraphics[width=13cm]{v1.png}
    \caption{Illustrate the shape of Beta distribution}
    \label{fig:beta.curve}
\end{figure}


For \figref{fig:beta.curve} we can generalize to the Dirichlet distribution that low $$\boldm{\alpha}$$ leads to a very low sparse curve in middle with high value at both edges, and high $$\boldm{\alpha}$$ leads to a high peak. These are the things most of Dirichlet distribution papers uses the simplex to illustrate the density of Dirichlet distribution without explaining the reason behind, this perhaps creating confusion for the reader. This writting of Dirichlet distribution helps for a better understanding of the intuition of Dirichlet distribution and reading LDA papers. Next, we use Dirichlet distribution to formulate LDA's model.

\subsection{LDA's model}
Let $$k$$ be the latent number of topics, $$D$$ be the number of documents and $$V$$ be the number of terms in the vocabulary. Note that we use $$k$$ not $$K$$ because it's a variable we will sample from the Dirichlet distribution. We use $$\alpha$$ to draw a the distribution of $$k$$ topics over documents $$d$$. And use $$\beta$$ to draw a distribution of topics over words $$w_d$$. We use $$i$$ to index a topic, $$d$$ to index a document, $$j$$ to index a word and $$w$$ to denote a word. In LDA, $$\alpha_{k\times1}$$ and $$\beta_{k\times V}$$ are model parameters, while $$\theta_{D\times k}$$ and $$\boldm{z}$$ are hidden variables.   
% We can see that $$\beta$$ has three levels of distribution:  
% \begin{itemize}
%     \item $$\beta$$ itself is a distribution of all its $$\beta_i$$, 
%     \item $$\beta_i$$ is a distribution of topic $$i$$, 
%     \item $$\beta_{ij}$$ is the probability of word $$j$$ in $$V$$ has the $$i$$th topic. 
% \end{itemize}
% we can say $$\beta_i$$ represents the multinomial distribution of word $$j$$ for the $$i$$th topic, 
Given these distributions, the LDA generative process is as follows:

\begin{enumerate}
    \item For each documents:
     \begin{enumerate}
        \item Choose a distribution $$d_1$$ over topics, for example \enquote{70\% topic A, 20\% topic B, 10\% topic C}.
        \item For each word in the document:
        \begin{enumerate}
            \item Draw a topic $$i$$ in one of the $$k$$ topics with a distribution $$d_2$$ conditioned on $$d_1$$. For example, if we pick topic A, the document will have 70\% chance of containing topic A.
            \item Draw a word amongst many words in topic $$i$$ with a distibution $$d_3$$ conditioned on $$d_2$$.
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

This model shows that:
\begin{itemize}
    \item A document contains a mixture of topics. For example, document $$d$$ contains \enquote{70\% topic A, 20\% topic B, 10\% topic C}.
    \item A topic contains a mixture of words. \enquote{Education} might contain words like \enquote{school}, \enquote{teachers}, \enquote{elementary}.
\end{itemize}

After having a rough understanding, we formalize the above process as follows:

\begin{enumerate}
    \item For each topic $$i$$ in $$k$$
    \begin{enumerate}
        \item Draw a topic distribution over words $$\phi_i \sim$$ Dir($$\beta$$). 
    \end{enumerate}
    \item For each word $$n$$ in the document $$d$$:
     \begin{enumerate}
        \item Draw a topic distribution over documents, $$\theta_d \sim $$ Dir($$\alpha$$)  
        \item For each word in the document:
        \begin{enumerate}
            \item Draw a specific topic $$z_{d,n} \sim$$ multi($$\theta$$)
            \item Draw a word $$w_{d,n} \sim \beta_{z_{d,n}}$$,
        \end{enumerate}
    \end{enumerate}
\end{enumerate}
where $$\phi_i$$ the probability over the vocabulary generated by latent topic $$i$$, Dir($$\alpha$$) is a draw from a uniform Dirichlet distribution with parameter $$\alpha$$ and multi(.) is a Multinomial distribution. Note that $$w_{d,n} \sim   p(w_{d,n}|z_{d,n},\beta_i) = \beta_{z_{d,n},i} = \beta_{z_{d,n}}$$, because $$z_n$$ is already $$i$$ we have picked from the beginning. The subsript $$n,d$$ or $$d$$ is just to express that we're only considering the word $$n$$ and document $$d$$. 

\hfill \break
\textbf{Why Dirichlet?}
The LDA generative process has 3-levels of distribution. The distribution $$d_1$$ is actually a Dirichlet distribution. Similarly to the multinomial distribution, from the pmf itself $$\frac{n!}{\prod x_i}\prod p_i^{x_i}$$, each topic with probability $$p_i$$ can appear $$x_i$$ times in all the corpus $$D$$. And the author also stated that \enquote{The Dirichlet is a convenient distribution on the simplex — it is in the \textbf{exponential family}, has \textbf{finite dimensional} sufficient statistics, and is \textbf{conjugate to the multinomial distribution}}. 
% https://tex.stackexchange.com/questions/64371/direct-quotations-and-entire-paragraph-quotations
\blockquote{\enquote{For example, it models the probability of counts for rolling a $$k$$-sided dice $$n$$ times. For $$n$$ independent trials each of which leads to a success for exactly one of $$k$$ categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.} - Multinomial distribution, Wikipedia.}
where $$n$$ is the number of times all topics appear in a corpus, $$k$$ categorical is the number of topics. The multinomial distribution can express partially the distribution of topics in a corpus, and Dirichlet is conjugate to it.

\hfill \break
% https://www.quora.com/Latent-Dirichlet-Allocation-LDA-What-does-it-mean-by-Generating-a-word-from-a-multinomial-distribution-conditioned-on-the-topic/answer/Kripa-Chettiar?share=07418772&srid=Ozad
\textbf{What roles $$\alpha,\beta$$ play?} Consider if 3 topics have the same probability, we choose $$\alpha(10,10,10)$$, if we want the first topic to appear more, we choose $$\alpha(30,10,10)$$. We can see how $$\alpha$$  represents the topic mixtures. We then draw a sample from this Dir($$\alpha$$) to get a vector $$\theta$$ that sums up to 1. This $$\theta$$ will be the weights of a $$k$$-sided dice. If we throw the dice and get 3, then we will generate a word in 3th topic. To this point, we're able to understand the process from $$\alpha \rightarrow \theta \rightarrow z_{i=3}$$. Similarly, we draw $$\phi_i$$ from Dir($$\beta$$) and then throw $$V$$ sides dices with weight $$\phi$$ to get a 7th word in $$V$$ $$w_{j=7}$$. We infer a similar process $$\beta \rightarrow \phi \rightarrow w_{j=7}$$. Note that we sample $$\phi$$ from $$\beta$$ using \eqref{eq.4.12} that will be derived later. This is the reason the author didn't mention what to sample from $$\beta$$ in the beginning of the paper, and also it's not included in the graphical model in \figref{fig.1}, but only a direct connection from $$\beta \rightarrow w$$.

\hfill \break
Next, to solve the problem of LDA we need to compute:

\begin{equation} \label{eq.4.1}
    p(\theta,\boldm{z}|\boldm{w},\alpha,\beta) = \frac{p(\theta,\boldm{z},\boldm{w}|\alpha,\beta)}{ p(\boldm{w}|\alpha,\beta)}.
\end{equation}

However, this distribution is intractable to compute. That's where Variational Inference comes about. The idea of Variational Inference is to find and optimize a closest family to the posterior by making use of Jensen’s inequality to obtain an adjustable lower bound on the log likelihood. Because we are only interested in a family ($$\gamma,\phi$$) closest to ($$\theta, \boldm{z}$$), we can drop some edges and nodes of \figref{fig.1} to simplify \eqref{eq.4.1} as below:

\begin{equation} \label{eq.4.2}
    q(\theta,\boldm{z} | \gamma,\phi) = q(\theta|\gamma)\prod_{n=1}^{N}q(z_n|\phi_n)
\end{equation}

The main idea is that we use variational expectation-maximization (EM):
\begin{enumerate}
    \item (E-step) For each document, find the optimizing values of the variational parameters $$\{\gamma^\ast_d, \phi^\ast_d: d\in D\}$$. 
    \item (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $$\alpha$$ and $$\beta$$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.
\end{enumerate}

\subsection{E-step: optimizing values of the variational parameters $$\gamma_d, \phi_d$$}
\subsubsection{Find a function $$L$$  for the expectation of the log likelihood }
Let $$f$$ be a convex function, and let $$X$$ be a random variable. Then:
\begin{equation*}
    \text{E}[f(X)] \geq f(\text{E}X).
\end{equation*}

Using Jensen's inequality we have:

\begin{equation}
    \begin{align*}
        \text{log}p(\boldm{w}|\alpha,\beta) &= \text{log}\int\sum_{\boldm{z}}p(\theta, \boldm{z}, \boldm{w}|\alpha,\beta)d\theta \\
        &= \text{log}\int\sum_{\boldm{z}} \frac{p(\theta, \boldm{z}, \boldm{w}|\alpha,\beta)q(\theta,\boldm{z})}{q(\theta,\boldm{z})} d\theta \\
        &\geq \int \sum_{\boldm{z}} q(\theta,\boldm{z}) p(\theta, \boldm{z}, \boldm{w}|\alpha,\beta) q(\theta,\boldm{z}) - \int \sum_{\boldm{z}} q(\theta,\boldm{z}) q(\theta,\boldm{z}) \\
          &= \text{E}_q[\text{log}p(\theta, \boldm{z}, \boldm{w}|\alpha,\beta)] - \text{E}_q[\text{log}q(\theta, \boldm{z})]
    \end{align*}
    \label{eq:x=2=1+1}
\end{equation}

\hfill \break
The difference between LHS and RHS of Eq.(4.3) is the KL divergence between the variational $$q(\theta, \boldm{z}|\gamma, \phi)$$ and true posterior probability $$p(\theta, \boldm{z}, \boldm{w}|\alpha,\beta)$$. Let RHS of Eq.(4.3) $$\delequal$$ $$L(\gamma,\phi;\alpha,\beta)$$ we have

\begin{equation} \label{eq.4.4}
    \text{log}p(\boldm{w}|\alpha,\beta) = L(\gamma,\phi;\alpha,\beta) + D_{\text{KL}}(q(\theta, \boldm{z}|\gamma, \phi) \hspace{1mm}||\hspace{1mm} p(\theta, \boldm{z}, \boldm{w}|\alpha,\beta))
\end{equation}

This shows that maximize $$L(\gamma,\phi;\alpha,\beta) \equalhat$$ minimize $$D_{\text{KL}}$$. We need to find a optimal solutions where:
\begin{equation} \label{eq.4.3}
    (\gamma^\ast,\phi^\ast) = \text{arg} \min_{\gamma,\phi}  \text{D}(q(\theta,\mathbf{z}|\gamma,\phi) || p(\theta,\boldm{z}|\boldm{w},\alpha,\beta)).
\end{equation}

\subsubsection{Optimize $$L$$}
Expand \eqref{eq.4.4}:

\begin{equation} \label{eq.4.5}
    \begin{align*}
        L(\gamma,\phi;\alpha,\beta) &= \text{E}_q[\text{log}p(\theta |\alpha)] + \text{E}_q[\text{log}p(\boldm{z}|\theta)] + \text{E}_q[\text{log}p(\boldm{w}|\boldm{z},\beta)] \\
        &- \text{E}_q[\text{log}p(\theta)] - \text{E}_q[\text{log}p(\boldm{z})]    
    \end{align*}
\end{equation}

Consider the first term:
\begin{equation*}
    \begin{align*}
        & \text{E}_q[\text{log}p(\theta |\alpha)] \\
        &= \text{E}_q[\text{log}\Big(\text{exp}\Big\{\Big(\sum^k_{j=1}(\alpha_i -1)\text{log}\theta_i \Big)  \Big\} + \text{log}\Gamma \Big( \sum^k_{i=1} \alpha_i \Big)
    - \sum^k_{i=1}\text{log}\Gamma(\alpha_i) \Big\} \Big)] \\
    &= \Big(\sum^k_{i=1}(\alpha_i - 1)\text{E}_q[\text{log}\theta_i]\Big) + \text{log}\Gamma \Big( \sum^k_{i=1} \alpha_i \Big)
    - \sum^k_{i=1}\text{log}\Gamma(\alpha_i) \\
    &= \Bigg(\sum^k_{i=1}(\alpha_i - 1)\Big(\Psi(\alpha_i) - \Psi \Big(\sum^k_{j=1}\alpha_j\Big) \Big)\Bigg) + \text{log}\Gamma \Big( \sum^k_{i=1} \alpha_i \Big)
    - \sum^k_{i=1}\text{log}\Gamma(\alpha_i).
    \end{align*}
\end{equation*}

Let $$z_n$$ the topic assignment of the $$n$$th word in current document. $$z_{n,i} = 1$$ when the topic assignment is the $$i$$th topic, otherwise $$z_{n,i} = 0$$. $$p(z_n|\theta)$$ is simply $$\theta_i$$ when $$z_{n,i}=1$$. Let $$p(z_{n,i}|\theta_i) =  \theta_i^{z_{n,i}}$$ for the unique $$i$$ so that: 

\begin{equation} \label{eq.8.5}
    \begin{align*}
    \text{log}p(z_{n,i}|\theta_i) &= \text{log}\theta_i^{z_{n,i}} \\
    &= {z_{n,i}}\text{log}\theta_i.
    % &=
    % \begin{cases}
    %   1, & \text{if } z_{n,i} = 1 \\
    % 0, & \text{if } z_{n,i} = 0.
    % \end{cases}
    \end{align*}
\end{equation}

% \[
% \text{log}p(z_n|\theta) = 
%     \begin{cases}
%       1, & \text{if } z_{n,i} = 1 \\
%     0, & \text{if } z_{n,i} = 0.
%     \end{cases}
% \]

% $$\text{log}p(z_n|\theta) = \text{log}\theta_i^{z_{n,i}} = {z_{n,i}}\text{log}\theta_i$$ takes the value of 1 if $$z_{n,i} = 1$$ and $$0$$ otherwise. 
Expand the second term:

\begin{equation*}
    \begin{align*}
    \text{E}_q[\text{log}p(\boldm{z}|\theta)] &= \sum^N_{n=1}\text{E}_q[\text{log}p(\boldm{z}_n|\theta)]\\
    &= \sum^N_{n=1}\sum^k_{i=1}\text{E}_q[\text{log}p(\boldm{z}_{n,i}|\theta_i)]\\
    &= \sum^N_{n=1}\sum^k_{i=1}\text{E}_q[ \text{log}\theta_i^{z_{n,i}}] \\
    &= \sum^N_{n=1}\sum^k_{i=1}\text{E}_q[ z_{n,i}\text{log}\theta_i] \\
    &= \sum^N_{n=1}\sum^k_{i=1}\text{E}_q [z_{n,i}]\text{E}_q[\text{log}\theta_i]\\
    &= \sum^N_{n=1}\sum^k_{i=1}\phi_{ni} \Big( \Psi(\gamma_i) - \Psi \Big(\sum^k_{j=1}\gamma_i \Big) \Big).
    \end{align*}
\end{equation*}

% $$\text{log}p(\boldm{w|z},\beta) = \text{log}\beta_i^{z_{n,i}.w_{n,j}} = {z_{n,i}.w_{n,j}}\text{log}\theta_i$$.
Let $$\beta_{ij} = p(w_{n,j}=1|z_i=1)$$, recall that each $$w_n$$ is a vector of size $$V$$ with exactly one component equal to one; we can select the unique $$j$$ such that $$w_{n,j}$$ = 1. Constraint similiarly to \eqref{eq.8.5}, we have:

\begin{equation}
    \begin{align*}
    \text{log}p(\boldm{w|z},\beta) &= \text{log}\beta_{ij}^{z_{n,i}.w_{n,j}} \\
    &= {z_{n,i}.w_{n,j}}\text{log}\beta_{ij}.
%     &= \begin{cases}
%       1  ,& \text{if } z_{n,i}.w_{n,j} = 1 \\
%   0  ,& \text{if } z_{n,i}.w_{n,j} = 0.
%     \end{cases}
    \end{align*}
\end{equation}


Third term:
\begin{equation*}
    \begin{align*}
        \text{E}_q[\text{log}p(\boldm{w|z},\beta) &= \sum^N_{n=1} \text{E}_q[\text{log}p(w_n|z_n,\beta)] \\
         &= \sum^N_{n=1}\sum^k_{i=1}\sum^V_{j=1}\text{E}_q[\text{log}p(w_{n,j}|z_{n,i},\beta_{ij})]\\
        &= \sum^N_{n=1}\sum^k_{i=1}\sum^V_{j=1}\text{E}_q[\text{log}\beta_{ij}^{z_{n,i}.w_{n,j}}] \\
        &=  \sum^N_{n=1}\sum^k_{i=1}\sum^V_{j=1}\text{E}_q[z_{n,i}]\text{E}_q[w_{n,j}]\text{E}_q[\text{log}\beta_{ij}]\\
        &= \sum^N_{n=1}\sum^k_{i=1}\sum^V_{j=1} \phi_{n,i}w_{n,j}\text{log}\beta_{ij}.\\
    \end{align*}
\end{equation*}

Expand the fourth term similarly the first term (just change $$\alpha$$ to $$\gamma$$) and fifth terms, we have:
\begin{equation*}
    \begin{align*}
    &- \text{E}_q[\text{log}p(\theta)] - \text{E}_q[\text{log}p(\boldm{z})] \\
    &= \text{E}_q[\text{log}p(\theta|\gamma)] - \text{E}_q[\text{log}p(\boldm{z}|\phi)] \\ 
    &= -\Bigg(\sum^k_{i=1}(\gamma_i - 1)\Big(\Psi(\gamma) - \Psi \Big(\sum^k_{j=1}\gamma_j\Big) \Big)\Bigg) + \text{log}\Gamma \Big( \sum^k_{i=1} \gamma_i \Big)
    - \sum^k_{i=1}\text{log}\Gamma(\gamma_i) \\
    &- \sum^N_{n=1}\sum^k_{i=1}\phi_{ni}\text{log}\phi_{ni}.
    \end{align*}
\end{equation*}
% \hfill \break
% Replace RHS of \eqref{eq.8.4} to $$\text{log}\theta_i$$ in \eqref{eq.8.2} we have $$p(\theta|\alpha)$$, therefore easily infer $$\text{E}_q[\text{log}p(\theta |\alpha)]$$. We can also infer other terms of \eqref{eq.4.5} that include $$\text{log}\theta_i$$. Expand the equation above, each line corresponds to each of five terms of \eqref{eq.4.5}:

Sum up all of those five terms, we have the lower bound:
\begin{equation}
    \begin{align*}
        L(\gamma,\phi;\alpha,\beta) &= \text{log}\Gamma \Big(\sum^k_{j=1}\alpha_i \Big) - \sum^k_{j=1}\text{log}\Gamma(\alpha_i) + \sum^k_{j=1}(\alpha_i - 1) \Big( \Psi(\gamma_i) - \Psi \Big(\sum^k_{j=1}\gamma_i \Big) \Big)  \\ 
        &+ \sum^N_{n=1}\sum^k_{i=1}\phi_{ni} \Big( \Psi(\gamma_i) - \Psi \Big(\sum^k_{j=1}\gamma_i \Big) \Big) \\
        &+ \sum^N_{n=1}\sum^k_{i=1}\sum^V_{j=1}\phi_{ni}w_{nj}\text{log}\beta_{ij} \\
        &- \text{log}\Gamma \Big(\sum^k_{j=1}\gamma_i \Big) + \sum^k_{i=1}\text{log}\Gamma(\gamma_i) - \sum^k_{i=1}(\gamma_i - 1) \Big( \Psi(\gamma_i) - \Psi \Big(\sum^k_{j=1}\gamma_i \Big) \Big) \\
        &- \sum^N_{n=1}\sum^k_{i=1}\phi_{ni}\text{log}\phi_{ni}.
    \end{align*}
\end{equation}

We then maximize the lower bound $$L(\gamma,\phi;\alpha,\beta)$$ w.r.t $$\phi$$ and $$\gamma$$.

\subsubsection*{Maximize $$L$$ w.r.t $$\phi$$}

$$\phi_{ni}$$ is the probability the $$n$$th word is generated by latent topic $$i$$, observed that $$\sum^k_{i=1}\phi_{ni} = 1$$. We only keep terms in $$L$$ that have $$\phi_{ni}$$ and ignore those with only $$\alpha_i$$ or $$\gamma_i$$. Using Lagrange multiplier, we can rewrite Eq.(8.4):

\begin{equation*}
    L_{[\phi_{ni}]} = \phi_{ni}\Big(\Psi(\gamma_i) - \Psi \Big(\sum^k_{j=1}\gamma_i \Big)  \Big) + \phi_{ni}\text{log}\beta_{ij} - \phi_{ni}\text{log}\phi_{ni} + \underbrace{\lambda \Big(\sum^k_{j=1}\phi_{ni} - 1 \Big)}_\text{Lagrange term}
\end{equation*}

Taking derivative of $$L$$ w.r.t $$\phi_{ni}$$:
\begin{equation*}
    \frac{\partial L}{\partial \phi_{ni}} = \Psi(\gamma_i) - \Psi \Big(\sum^k_{j=1}\gamma_i \Big) + \text{log}\beta_{ij} - \text{log}\phi_{ni} - 1 + \lambda
\end{equation*}

Set this derivative to zero, we have maximum value of $$\phi_{ni}$$:

\begin{equation}
    \phi_{ni} \propto \beta_{iw_n}\text{exp}\Big(\Psi(\gamma_i) - \Psi\Big(\sum^k_{j=1}\gamma_i \Big)\Big) 
\end{equation}

\subsubsection*{Maximize $$L$$ w.r.t $$\gamma$$}
Keep terms in Eq.(8.4) that contain $$\gamma_i$$:

\begin{equation*}
    \begin{align*}
        L_{[\gamma]} &= \sum^k_{i=1}( \alpha_i - 1 ) \Big(\Psi(\gamma_i) - \Psi\Big(\sum^k_{j=1}\gamma_i \Big)\Big) + \sum^N_{n=1}\phi_{ni}\Big(\Psi(\gamma_i) - \Psi\Big(\sum^k_{j=1}\gamma_i \Big)\Big) \\
        &- \text{log}\Gamma \Big( \sum^k_{j=1}\gamma_i \Big) + \text{log}\Gamma(\gamma_i) - \sum^k_{i=1}(\gamma_i - 1) \Big(\Psi(\gamma_i) - \Psi\Big(\sum^k_{j=1}\gamma_i \Big)\Big) \\
        &= \sum^k_{i=1} \Big(\Psi(\gamma_i) - \Psi\Big(\sum^k_{j=1}\gamma_i \Big)\Big) \Big( \alpha_i + \sum^N_{n=1}\phi_{ni} - \gamma_i \Big) - \text{log}\Gamma \Big(\sum^k_{j=1}\gamma_i \Big) + \text{log}\Gamma(\gamma_i)
    \end{align*}
\end{equation*}

Taking derivative of $$L_{[\gamma]}$$ w.r.t $$\gamma_i$$:

\begin{equation*}
    \frac{\partial L}{\partial \gamma_i} = \Psi'(\gamma_i) \Big( \alpha_i + \sum^N_{n=1}\phi_{ni} - \gamma_i \Big) - \Psi'\Big( \sum^k_{j=1} \gamma_i \Big) \sum^k_{j=1} \Big( \alpha_i + \sum^N_{n=1}\phi_{ni} - \gamma_i \Big)
\end{equation*}

$$L_{[\gamma]}$$ is maximum when $$\frac{\partial L}{\partial \gamma_i} = 0$$ at:
\begin{equation}
    \gamma_i = \alpha_i + \sum^N_{n=1}\phi_{ni}
\end{equation}

\subsubsection{Conlusion of E-step}
The optimal $$\gamma^\ast,\phi^\ast$$ is found: 
\begin{equation} \label{eq.4.12}
    \phi_{ni} \propto \beta_{iw_n}\text{exp}\{\text{E}_q[\text{log}\theta_i|\gamma]\}
\end{equation}

\begin{equation}
    \gamma_i = \alpha_i + \sum^N_{n=1}\phi_{ni}
\end{equation}

Where

\begin{equation*}
    \text{E}_q[\text{log}\theta_i|\gamma] = \Psi(\gamma_i) - \Psi \Big(\sum^k_{j=1}\gamma_j\Big)
\end{equation*}

with $$\Psi = \frac{d}{dx}\text{log}(\Gamma(x))$$,   the digamma function.

\subsection{M-step: maximizing the lower bound with respect to parameters $$\alpha, \beta$$}
With the set of documents $$D$$ $$\{\boldm{w}_d\}^M_{d=1}$$ we wish to find parameters $$\alpha$$ and $$\beta$$ that maximize the log likelihood:

\begin{equation}
    \ell(\alpha, \beta) = \sum^M_{d=1}\text{log}p(\boldm{w}_d|\alpha, \beta)
\end{equation}

To maximize with respect to $$\beta$$, we isolate terms and add Lagrange multipliers:
\begin{equation*}
    L_{[\beta]} = \sum^M_{d=1}\sum^{N_d}_{n=1}\sum^{k}_{i=1}\sum^{V}_{j=1}\phi_{dni}w_{dnj}\text{log}\beta_{ij} + \sum^k_{i=1}\lambda_i\Big( \sum^V_{j=1}\beta_{ij} - 1 \Big).
\end{equation*}

Take derivative w.r.t $$\beta_{ij}$$, set it to zero, and find:
\begin{equation*}
    \beta_{ij} \propto \sum^M_{d=1}\sum^{N_d}_{n=1}\phi_{dni}w_{dnj}.
\end{equation*}

The terms which contain $$\alpha$$ are:
\begin{equation*}
    L_{[\alpha]} = \sum^M_{d=1}\Bigg(\text{log}\Gamma(\sum^k_{j=1}\alpha_j) - \sum^k_{i=1}\text{log}\Gamma(\alpha_i) + \sum^k_{i=1}\Big((\alpha_i-1) \Big( \Psi(\gamma_{di}) - \Psi \Big(\sum^k_{j=1}\gamma_{dj}\Big)\Big)  \Big) \Bigg)
\end{equation*}

Taking the derivative w.r.t $$\alpha_i$$:
\begin{equation*}
    \frac{\partial L}{\partial \alpha_i} = M\Big(\Psi\Big(\sum^k_{j=1}\alpha_i \Big) - \Psi(\alpha_i) \Big) + \sum^M_{d=1}\Big(\Psi(\gamma_{di}) - \Psi \Big(\sum^k_{j=1}\gamma_{dj}\Big) \Big)
\end{equation*}

This derivative depends on $$\alpha_i$$, where $$j \neq i$$, and we therefore must use an iterative method to find the maximal $$\alpha$$ by linear-scaling Newton-Rhapson algorithm. In particular, the Hessian is in the form:

\begin{equation} \label{eq.hessian}
    \frac{\partial^2 L}{\partial^2 \alpha_i} = \delta(i,j)M\Psi'(\alpha_i) - \Psi'\Big(\sum^k_{j=1}\alpha_i \Big).
\end{equation}

\begin{algorithm}
  \caption{Variational Expectation-Maximization for LDA}\label{VI}
  
  \textbf{Input:} $$k$$ topics, $$M$$ documents and $$N_d$$ words.
  
  \textbf{Output:} Model parameter: $$\beta,\theta,z$$
  
  \begin{algorithmic}[1]
  \Initialize{$$\phi^0_{ni}:= 1/k$$ for all $$i$$ in $$k$$ and $$n$$ in $$N_d$$}
  \Initialize{$$\gamma_i := \alpha + N/k$$ for all $$i$$ in $$k$$}
  \Initialize{$$\alpha_i:= 50/k$$}
  \Initialize{$$\beta_{ij}:=0$$ for all $$i$$ in $$k$$ and $$j$$ in $$V$$}
  //E-step: optimizing values of the variational parameters $$\gamma_d, \phi_d$$    
  \State loglikelihood $$:=0$$
  \For{$$d = 1$$ to $$M$$}
  \Repeat
    \For{$$n = 1$$ to $$N$$} 
     \For{$$i = 1$$ to $$k$$}
        \State $$\phi_{ni}^{t+1}:=\beta_{iw_n} \text{exp}(\Psi(\gamma_i^t))$$
    \EndFor
    \State normalize $$\phi^{t+1}_n$$ to sum to 1
  \EndFor
  
  \State $$\gamma^{t+1} := \alpha + \sum^N_{n=1}\phi^{t+1}_n$$
  \Until{convergence of $$\gamma_d, \phi_d$$}
  
 \State loglikelihood: = loglikelihood + $$L(\gamma,\phi;\alpha,\beta)$$
  \EndFor
  
  //M-step: maximizing the lower bound with respect to parameters $$\alpha, \beta$$
  \For{$$n = 1$$ to $$M$$}
    \For{$$i = 1$$ to $$k$$}
     \For{$$j = 1$$ to $$V$$}    
        \State $$\beta_{ij}:=\phi_{dni}w_{dnj}$$
      \EndFor
      
      \State normalize $$\beta_i$$ to sum to 1
     \EndFor
  \EndFor
  
 \State estimate $$\alpha$$ by \eqref{eq.hessian}
  
  \If{loglikelihood converged}
  \State return parameters
  \Else
   \State go back to E-step
  \EndIf
  \end{algorithmic}
  
\end{algorithm}
\subsection{Why LDA works?}
Allowing topics sparsity and penalize too many topics of a document:
\begin{itemize}
    \item When $$\alpha < 1$$, the majority of the probability mass is in the two side-edges of the distribution, generating mostly documents that have only a small number of topics.

    \item When $$\alpha > 1$$, most documents will have almost every topic.
\end{itemize}

LDA posterior put “topical” words together because of word co-occurences:
\begin{itemize}
    \item Word probabilities are maximized by dividing the words among the topics. (More terms means more mass to be spread around.)
    \item In a mixture, this is enough to find clusters of co-occurring words.
\end{itemize}

The joint distribution allows our above statements to happen:
\begin{equation*}
    p(\theta,\boldm{z}|\boldm{w},\alpha,\beta ) \propto p(\theta, \boldm{z}|\boldm{w}|\alpha,\beta) = \underbrace{p(\boldm{w}|\boldm{z},\beta)}_\text{1} \underbrace{p(\boldm{z}|\theta)}_\text{2} \underbrace{p(\theta|\alpha)}_\text{3}
\end{equation*}

\begin{enumerate}
    \item Encourage a sparse $$\beta$$ in order to increase the probability of certain words $$w$$.
    \item Encourage a sparse $$\theta$$ in order to increase the probability of certain topic $$z$$.
    \item Using a small $$\alpha$$ will increase the probability of $$\theta$$.
\end{enumerate}
(1) wants to give form a sparse word clusters, (2) and (3) want to give a small number of topics to a document. The joint distribution encourages a small number of topics but the data itself needs a larger number of topics in order to assign to each document and also to form the diverse topics of all documents. So, as desired, we have a push and pull factor between the joint distribution and the actual data in determining the assignment of topics.

\subsection{LDA's code implementation} \label{code}

% \lstset
% { %Formatting for code in appendix
%     language=python,
%     basicstyle=\footnotesize,
%     numbers=left,
%     stepnumber=1,
%     showstringspaces=false,
%     tabsize=1,
%     breaklines=true,
%     breakatwhitespace=false,
% }
The code is written in C language, Copyright 2004, David M. Blei (blei [at] cs [dot] cmu [dot] edu) and forked from \href{https://github.com/blei-lab/lda-c}{\underline{GitHub}}. 

\hfill \break
The code will be explained by top down manner. The main program is directly straighforward, run EM or just inference.
\lstinputlisting[linerange={314-349}, caption=\textbf{lda-estimate.c},language=c]{Code/lda-estimate.c}

EM algorithm is straighforward, here we only include important lines, for more details you can look at the entire lda-estimate.c . This step corresponds to line 26-29 in Algorithm~\ref{VI}:
\lstinputlisting[linerange={111-116,169-200}, caption=\textbf{lda-estimate.c},language=c]{Code/lda-estimate.c}

This step corresponds to line 16 in Algorithm~\ref{VI}, after the convergence of $$\gamma_d, \phi_d$$:
\lstinputlisting[linerange={27-59}, caption=\textbf{lda-estimate.c},language=c]{Code/lda-estimate.c}

This step corresponds to line 6-15 in Algorithm~\ref{VI}:
\lstinputlisting[linerange={27-85}, caption=\textbf{lda-inference.c},language=c]{Code/lda-inference.c}

This step calculate $$L(\gamma,\phi;\alpha,\beta)$$:
\lstinputlisting[linerange={88-128}, caption=\textbf{lda-inference.c},language=c]{Code/lda-inference.c}

By understanding the above code, the reader can walk through the other files and the entire source code of the implementation without much difficulty.

This is a long overdue blog post on Reinforcement Learning (RL). RL is hot! You may have noticed that computers can now automatically [learn to play ATARI games](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html) (from raw game pixels!), they are beating world champions at [Go](http://googleresearch.blogspot.com/2016/01/alphago-mastering-ancient-game-of-go.html), simulated quadrupeds are learning to [run and leap](https://www.cs.ubc.ca/~van/papers/2016-TOG-deepRL/index.html), and robots are learning how to perform [complex manipulation tasks](http://www.bloomberg.com/features/2015-preschool-for-robots/) that defy explicit programming. It turns out that all of these advances fall under the umbrella of RL research. I also became interested in RL myself over the last ~year: I worked [through Richard Sutton's book](https://webdocs.cs.ualberta.ca/~sutton/book/the-book.html), read through [David Silver's course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), watched [John Schulmann's lectures](https://www.youtube.com/watch?v=oPGVsoBonLM), wrote an [RL library in Javascript](http://cs.stanford.edu/people/karpathy/reinforcejs/), over the  summer interned at DeepMind working in the DeepRL group, and most recently pitched in a little with the design/development of [OpenAI Gym](https://gym.openai.com/), a new RL benchmarking toolkit. So I've certainly been on this funwagon for at least a year but until now I haven't gotten around to writing up a short post on why RL is a big deal, what it's about, how it all developed and where it might be going.

<div class="imgcap">
<img src="/assets/rl/preview.jpeg">
<div class="thecap">Examples of RL in the wild. <b>From left to right</b>: Deep Q Learning network playing ATARI, AlphaGo, Berkeley robot stacking Legos, physically-simulated quadruped leaping over terrain.</div>
</div>

It's interesting to reflect on the nature of recent progress in RL. I broadly like to think about four separate factors that hold back AI:

1. Compute (the obvious one: Moore's Law, GPUs, ASICs), 
2. Data (in a nice form, not just out there somewhere on the internet - e.g. ImageNet), 
3. Algorithms (research and ideas, e.g. backprop, CNN, LSTM), and 
4. Infrastructure (software under you - Linux, TCP/IP, Git, ROS, PR2, AWS, AMT, TensorFlow, etc.). 

Similar to what happened in Computer Vision, the progress in RL is not driven as much as you might reasonably assume by new amazing ideas. In Computer Vision, the 2012 AlexNet was mostly a scaled up (deeper and wider) version of 1990's ConvNets. Similarly, the ATARI Deep Q Learning paper from 2013 is an implementation of a standard algorithm (Q Learning with function approximation, which you can find in the standard RL book of Sutton 1998), where the function approximator happened to be a ConvNet. AlphaGo uses policy gradients with Monte Carlo Tree Search (MCTS) - these are also standard components. Of course, it takes a lot of skill and patience to get it to work, and multiple clever tweaks on top of old algorithms have been developed, but to a first-order approximation the main driver of recent progress is not the algorithms but (similar to Computer Vision) compute/data/infrastructure.

Now back to RL. Whenever there is a disconnect between how magical something seems and how simple it is under the hood I get all antsy and really want to write a blog post. In this case I've seen many people who can't believe that we can automatically learn to play most ATARI games at human level, with one algorithm, from pixels, and from scratch - and it is amazing, and I've been there myself! But at the core the approach we use is also really quite profoundly dumb (though I understand it's easy to make such claims in retrospect). Anyway, I'd like to walk you through Policy Gradients (PG), our favorite default choice for attacking RL problems at the moment. If you're from outside of RL you might be curious why I'm not presenting DQN instead, which is an alternative and better-known RL algorithm, widely popularized by the [ATARI game playing paper](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html). It turns out that Q-Learning is not a great algorithm (you could say that DQN is so 2013 (okay I'm 50% joking)). In fact most people prefer to use Policy Gradients, including the authors of the original DQN paper who have [shown](http://arxiv.org/abs/1602.01783) Policy Gradients to work better than Q Learning when tuned well. PG is preferred because it is end-to-end: there's an explicit policy and a principled approach that directly optimizes the expected reward. Anyway, as a running example we'll learn to play an ATARI game (Pong!) with PG, from scratch, from pixels, with a deep neural network, and the whole thing is 130 lines of Python only using numpy as a dependency ([Gist link](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5)). Lets get to it.

### Pong from pixels

<div class="imgcap">
<div style="display:inline-block">
	<img src="/assets/rl/pong.gif">
</div>
<div style="display:inline-block; margin-left: 20px;">
	<img src="/assets/rl/mdp.png" height="206">
</div>
<div class="thecap"><b>Left:</b> The game of Pong. <b>Right:</b> Pong is a special case of a <a href="https://en.wikipedia.org/wiki/Markov_decision_process">Markov Decision Process (MDP)</a>: A graph where each node is a particular game state and each edge is a possible (in general probabilistic) transition. Each edge also gives a reward, and the goal is to compute the optimal way of acting in any state to maximize rewards.</div>
</div>

The game of Pong is an excellent example of a simple RL task. In the ATARI 2600 version we'll use you play as one of the paddles (the other is controlled by a decent AI) and you have to bounce the ball past the other player (I don't really have to explain Pong, right?). On the low level the game works as follows: we receive an image frame (a `210x160x3` byte array (integers from 0 to 255 giving pixel values)) and we get to decide if we want to move the paddle UP or DOWN (i.e. a binary choice). After every single choice the game simulator executes the action and gives us a reward: Either a +1 reward if the ball went past the opponent, a -1 reward if we missed the ball, or 0 otherwise. And of course, our goal is to move the paddle so that we get lots of reward.

As we go through the solution keep in mind that we'll try to make very few assumptions about Pong because we secretly don't really care about Pong; We care about complex, high-dimensional problems like robot manipulation, assembly and navigation. Pong is just a fun toy test case, something we play with while we figure out how to write very general AI systems that can one day do arbitrary useful tasks.

**Policy network**. First, we're going to define a *policy network* that implements our player (or "agent"). This network will take the state of the game and decide what we should do (move UP or DOWN). As our favorite simple block of compute we'll use a 2-layer neural network that takes the raw image pixels (100,800 numbers total (210\*160\*3)), and produces a single number indicating the probability of going UP. Note that it is standard to use a *stochastic* policy, meaning that we only produce a *probability* of moving UP. Every iteration we will sample from this distribution (i.e. toss a biased coin) to get the actual move. The reason for this will become more clear once we talk about training. 

<div class="imgcap">
<img src="/assets/rl/policy.png" height="200">
<div class="thecap">Our policy network is a 2-layer fully-connected net.</div>
</div>

and to make things concrete here is how you might implement this policy network in Python/numpy. Suppose we're given a vector `x` that holds the (preprocessed) pixel information. We would compute:

```python
h = np.dot(W1, x) # compute hidden layer neuron activations
h[h<0] = 0 # ReLU nonlinearity: threshold at zero
logp = np.dot(W2, h) # compute log probability of going up
p = 1.0 / (1.0 + np.exp(-logp)) # sigmoid function (gives probability of going up)
```

where in this snippet `W1` and `W2` are two matrices that we initialize randomly. We're not using biases because meh. Notice that we use the *sigmoid* non-linearity at the end, which squashes the output probability to the range [0,1]. Intuitively, the neurons in the hidden layer (which have their weights arranged along the rows of `W1`) can detect various game scenarios (e.g. the ball is in the top, and our paddle is in the middle), and the weights in `W2` can then decide if in each case we should be going UP or DOWN. Now, the initial random `W1` and `W2` will of course cause the player to spasm on spot. So the only problem now is to find `W1` and `W2` that lead to expert play of Pong!

*Fine print: preprocessing.* Ideally you'd want to feed at least 2 frames to the policy network so that it can detect motion. To make things a bit simpler (I did these experiments on my Macbook) I'll do a tiny bit of preprocessing, e.g. we'll actually feed *difference frames* to the network (i.e. subtraction of current and last frame).

**It sounds kind of impossible**. At this point I'd like you to appreciate just how difficult the RL problem is. We get 100,800 numbers (210\*160\*3) and forward our policy network (which easily involves on order of a million parameters in `W1` and `W2`). Suppose that we decide to go UP. The game might respond that we get 0 reward this time step and gives us another 100,800 numbers for the next frame. We could repeat this process for hundred timesteps before we get any non-zero reward! E.g. suppose we finally get a +1. That's great, but how can we tell what made that happen? Was it something we did just now? Or maybe 76 frames ago? Or maybe it had something to do with frame 10 and then frame 90? And how do we figure out which of the million knobs to change and how, in order to do better in the future? We call this the *credit assignment problem*. In the specific case of Pong we know that we get a +1 if the ball makes it past the opponent. The *true* cause is that we happened to bounce the ball on a good trajectory, but in fact we did so many frames ago - e.g. maybe about 20 in case of Pong, and every single action we did afterwards had zero effect on whether or not we end up getting the reward. In other words we're faced with a very difficult problem and things are looking quite bleak.

**Supervised Learning**. Before we dive into the Policy Gradients solution I'd like to remind you briefly about supervised learning because, as we'll see, RL is very similar. Refer to the diagram below. In ordinary supervised learning we would feed an image to the network and get some probabilities, e.g. for two classes UP and DOWN. I'm showing log probabilities (-1.2, -0.36) for UP and DOWN instead of the raw probabilities (30% and 70% in this case) because we always optimize the log probability of the correct label (this makes math nicer, and is equivalent to optimizing the raw probability because log is monotonic). Now, in supervised learning we would have access to a label. For example, we might be told that the correct thing to do right now is to go UP (label 0). In an implementation we would enter gradient of 1.0 on the log probability of UP and run backprop to compute the gradient vector \\(\nabla_{W} \log p(y=UP \mid x) \\). This gradient would tell us how we should change every one of our million parameters to make the network slightly more likely to predict UP. For example, one of the million parameters in the network might have a gradient of -2.1, which means that if we were to increase that parameter by a small positive amount (e.g. `0.001`), the log probability of UP would decrease by `2.1 * 0.001` (decrease due to the negative sign). If we then did a parameter update then, yay, our network would now be slightly more likely to predict UP when it sees a very similar image in the future.

<div class="imgcap">
<img src="/assets/rl/sl.png">
</div>

**Policy Gradients**. Okay, but what do we do if we do not have the correct label in the Reinforcement Learning setting? Here is the Policy Gradients solution (again refer to diagram below). Our policy network calculated probability of going UP as 30% (logprob -1.2) and DOWN as 70% (logprob -0.36). We will now sample an action from this distribution; E.g. suppose we sample DOWN, and we will execute it in the game. At this point notice one interesting fact: We could immediately fill in a gradient of 1.0 for DOWN as we did in supervised learning, and find the gradient vector that would encourage the network to be slightly more likely to do the DOWN action in the future. So we can immediately evaluate this gradient and that's great, but the problem is that at least for now we do not yet know if going DOWN is good. But the critical point is that that's okay, because we can simply wait a bit and see! For example in Pong we could wait until the end of the game, then take the reward we get (either +1 if we won or -1 if we lost), and enter that scalar as the gradient for the action we have taken (DOWN in this case). In the example below, going DOWN ended up to us losing the game (-1 reward). So if we fill in -1 for log probability of DOWN and do backprop we will find a gradient that *discourages* the network to take the DOWN action for that input in the future (and rightly so, since taking that action led to us losing the game).

<div class="imgcap">
<img src="/assets/rl/rl.png">
</div>

And that's it: we have a stochastic policy that samples actions and then actions that happen to eventually lead to good outcomes get encouraged in the future, and actions taken that lead to bad outcomes get discouraged. Also, the reward does not even need to be +1 or -1 if we win the game eventually. It can be an arbitrary measure of some kind of eventual quality. For example if things turn out really well it could be 10.0, which we would then enter as the gradient instead of -1 to start off backprop. That's the beauty of neural nets; Using them can feel like cheating: You're allowed to have 1 million parameters embedded in 1 teraflop of compute and you can make it do arbitrary things with SGD. It shouldn't work, but amusingly we live in a universe where it does.

**Training protocol.** So here is how the training will work in detail. We will initialize the policy network with some `W1`, `W2` and play 100 games of Pong (we call these policy "rollouts"). Lets assume that each game is made up of 200 frames so in total we've made 20,000 decisions for going UP or DOWN and for each one of these we know the parameter gradient, which tells us how we should change the parameters if we wanted to encourage that decision in that state in the future. All that remains now is to label every decision we've made as good or bad. For example suppose we won 12 games and lost 88. We'll take all 200\*12 = 2400 decisions we made in the winning games and do a positive update (filling in a +1.0 in the gradient for the sampled action, doing backprop, and parameter update encouraging the actions we picked in all those states). And we'll take the other 200\*88 = 17600 decisions we made in the losing games and do a negative update (discouraging whatever we did). And... that's it. The network will now become slightly more likely to repeat actions that worked, and slightly less likely to repeat actions that didn't work. Now we play another 100 games with our new, slightly improved policy and rinse and repeat.

> Policy Gradients: Run a policy for a while. See what actions led to high rewards. Increase their probability.

<div class="imgcap">
<img src="/assets/rl/episodes.png">
<div class="thecap" style="text-align:justify;">Cartoon diagram of 4 games. Each black circle is some game state (three example states are visualized on the bottom), and each arrow is a transition, annotated with the action that was sampled. In this case we won 2 games and lost 2 games. With Policy Gradients we would take the two games we won and slightly encourage every single action we made in that episode. Conversely, we would also take the two games we lost and slightly discourage every single action we made in that episode.</div>
</div>

If you think through this process you'll start to find a few funny properties. For example what if we made a good action in frame 50 (bouncing the ball back correctly), but then missed the ball in frame 150? If every single action is now labeled as bad (because we lost), wouldn't that discourage the correct bounce on frame 50? You're right - it would. However, when you consider the process over thousands/millions of games, then doing the first bounce correctly makes you slightly more likely to win down the road, so on average you'll see more positive than negative updates for the correct bounce and your policy will end up doing the right thing.

**Update: December 9, 2016 - alternative view**. In my explanation above I use the terms such as "fill in the gradient and backprop", which I realize is a special kind of thinking if you're used to writing your own backprop code, or using Torch where the gradients are explicit and open for tinkering. However, if you're used to Theano or TensorFlow you might be a little perplexed because the code is oranized around specifying a loss function and the backprop is fully automatic and hard to tinker with. In this case, the following alternative view might be more intuitive. In vanilla supervised learning the objective is to maximize \\( \sum\_i \log p(y\_i \mid x\_i) \\) where \\(x\_i, y\_i \\) are training examples (such as images and their labels). Policy gradients is exactly the same as supervised learning with two minor differences: 1) We don't have the correct labels \\(y\_i\\) so as a "fake label" we substitute the action we happened to sample from the policy when it saw \\(x\_i\\), and 2) We modulate the loss for each example multiplicatively based on the eventual outcome, since we want to increase the log probability for actions that worked and decrease it for those that didn't. So in summary our loss now looks like \\( \sum\_i A\_i \log p(y\_i \mid x\_i) \\), where \\(y\_i\\) is the action we happened to sample and \\(A_i\\) is a number that we call an **advantage**. In the case of Pong, for example, \\(A\_i\\) could be 1.0 if we eventually won in the episode that contained \\(x\_i\\) and -1.0 if we lost. This will ensure that we maximize the log probability of actions that led to good outcome and minimize the log probability of those that didn't. So reinforcement learning is exactly like supervised learning, but on a continuously changing dataset (the episodes), scaled by the advantage, and we only want to do one (or very few) updates based on each sampled dataset.

**More general advantage functions**. I also promised a bit more discussion of the returns. So far we have judged the *goodness* of every individual action based on whether or not we win the game. In a more general RL setting we would receive some reward \\(r_t\\) at every time step. One common choice is to use a discounted reward, so the "eventual reward" in the diagram above would become \\( R\_t = \sum\_{k=0}^{\infty} \gamma^k r\_{t+k} \\), where \\(\gamma\\) is a number between 0 and 1 called a discount factor (e.g. 0.99). The expression states that the strength with which we encourage a sampled action is the weighted sum of all rewards afterwards, but later rewards are exponentially less important. In practice it can can also be important to normalize these. For example, suppose we compute \\(R_t\\) for all of the 20,000 actions in the batch of 100 Pong game rollouts above. One good idea is to "standardize" these returns (e.g. subtract mean, divide by standard deviation) before we plug them into backprop. This way we're always encouraging and discouraging roughly half of the performed actions. Mathematically you can also interpret these tricks as a way of controlling the variance of the policy gradient estimator. A more in-depth exploration can be found [here](http://arxiv.org/abs/1506.02438).

**Deriving Policy Gradients**. I'd like to also give a sketch of where Policy Gradients come from mathematically. Policy Gradients are a special case of a more general *score function gradient estimator*. The general case is that when we have an expression of the form \\(E_{x \sim p(x \mid \theta)} [f(x)] \\) - i.e. the expectation of some scalar valued score function \\(f(x)\\) under some probability distribution \\(p(x;\theta)\\) parameterized by some \\(\theta\\). Hint hint, \\(f(x)\\) will become our reward function (or advantage function more generally) and \\(p(x)\\) will be our policy network, which is really a model for \\(p(a \mid I)\\), giving a distribution over actions for any image \\(I\\). Then we are interested in finding how we should shift the distribution (through its parameters \\(\theta\\)) to increase the scores of its samples, as judged by \\(f\\) (i.e. how do we change the network's parameters so that action samples get higher rewards). We have that:

$$
\begin{align}
        f(x;\alpha, \beta) &= \text{Beta}(\alpha, \beta) \\
        &=  \frac{1}{\text{B}(\alpha,\beta)} x^{\alpha -1}(1-x)^{\beta-1} \\
        &= \frac{ \Gamma\big(\alpha + \beta \big)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha -1}(1-x)^{\beta-1},\\
\end{align}
$$

To put this in English, we have some distribution \\(p(x;\theta)\\) (I used shorthand \\(p(x)\\) to reduce clutter) that we can sample from (e.g. this could be a gaussian). For each sample we can also evaluate the score function \\(f\\) which takes the sample and gives us some scalar-valued score. This equation is telling us how we should shift the distribution (through its parameters \\(\theta\\)) if we wanted its samples to achieve higher scores, as judged by \\(f\\). In particular, it says that look: draw some samples \\(x\\), evaluate their scores \\(f(x)\\), and for each \\(x\\) also evaluate the second term \\( \nabla\_{\theta} \log p(x;\theta) \\). What is this second term? It's a vector - the gradient that's giving us the direction in the parameter space that would lead to increase of the probability assigned to an \\(x\\). In other words if we were to nudge \\(\theta\\) in the direction of \\( \nabla\_{\theta} \log p(x;\theta) \\) we would see the new probability assigned to some \\(x\\) slightly increase. If you look back at the formula, it's telling us that we should take this direction and multiply onto it the scalar-valued score \\(f(x)\\). This will make it so that samples that have a higher score will "tug" on the probability density stronger than the samples that have lower score, so if we were to do an update based on several samples from \\(p\\) the probability density would shift around in the direction of higher scores, making highly-scoring samples more likely.

<div class="imgcap">
<img src="/assets/rl/pg.png">
<div class="thecap" style="text-align:justify;">
	A visualization of the score function gradient estimator. <b>Left</b>: A gaussian distribution and a few samples from it (blue dots). On each blue dot we also plot the gradient of the log probability with respect to the gaussian's mean parameter. The arrow indicates the direction in which the mean of the distribution should be nudged to increase the probability of that sample. <b>Middle</b>: Overlay of some score function giving -1 everywhere except +1 in some small regions (note this can be an arbitrary and not necessarily differentiable scalar-valued function). The arrows are now color coded because due to the multiplication in the update we are going to average up all the green arrows, and the <i>negative</i> of the red arrows. <b>Right</b>: after parameter update, the green arrows and the reversed red arrows nudge us to left and towards the bottom. Samples from this distribution will now have a higher expected score, as desired.
</div>
</div>

I hope the connection to RL is clear. Our policy network gives us samples of actions, and some of them work better than others (as judged by the advantage function). This little piece of math is telling us that the way to change the policy's parameters is to do some rollouts, take the gradient of the sampled actions, multiply it by the score and add everything, which is what we've done above. For a more thorough derivation and discussion I recommend [John Schulman's lecture](https://www.youtube.com/watch?v=oPGVsoBonLM).

**Learning**. Alright, we've developed the intuition for policy gradients and saw a sketch of their derivation. I implemented the whole approach in a [130-line Python script](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5), which uses [OpenAI Gym](https://gym.openai.com/)'s ATARI 2600 Pong. I trained a 2-layer policy network with 200 hidden layer units using RMSProp on batches of 10 episodes (each episode is a few dozen games, because the games go up to score of 21 for either player). I did not tune the hyperparameters too much and ran the experiment on my (slow) Macbook, but after training for 3 nights I ended up with a policy that is slightly better than the AI player. The total number of episodes was approximately 8,000 so the algorithm played roughly 200,000 Pong games (quite a lot isn't it!) and made a total of ~800 updates. I'm told by friends that if you train on GPU with ConvNets for a few days you can beat the AI player more often, and if you also optimize hyperparameters carefully you can also consistently dominate the AI player (i.e. win every single game). However, I didn't spend too much time computing or tweaking, so instead we end up with a Pong AI that illustrates the main ideas and works quite well:

<div style="text-align:center;">
<iframe width="420" height="315" src="https://www.youtube.com/embed/YOW8m2YGtRg?autoplay=1&amp;loop=1&amp;rel=0&amp;showinfo=0&amp;playlist=YOW8m2YGtRg" frameborder="0" allowfullscreen></iframe>
<br>
The learned agent (in green, right) facing off with the hard-coded AI opponent (left).
</div>

**Learned weights**. We can also take a look at the learned weights. Due to preprocessing every one of our inputs is an 80x80 difference image (current frame minus last frame). We can now take every row of `W1`, stretch them out to 80x80 and visualize. Below is a collection of 40 (out of 200) neurons in a grid. White pixels are positive weights and black pixels are negative weights. Notice that several neurons are tuned to particular traces of bouncing ball, encoded with alternating black and white along the line. The ball can only be at a single spot, so these neurons are multitasking and will "fire" for multiple locations of the ball along that line. The alternating black and white is interesting because as the ball travels along the trace, the neuron's activity will fluctuate as a sine wave and due to the ReLU it would "fire" at discrete, separated positions along the trace. There's a bit of noise in the images, which I assume would have been mitigated if I used L2 regularization.

<div class="imgcap">
<img src="/assets/rl/weights.png">
</div>

### What isn't happening

So there you have it - we learned to play Pong from from raw pixels with Policy Gradients and it works quite well. The approach is a fancy form of guess-and-check, where the "guess" refers to sampling rollouts from our current policy, and the "check" refers to encouraging actions that lead to good outcomes. Modulo some details, this represents the state of the art in how we currently approach reinforcement learning problems. Its impressive that we can learn these behaviors, but if you understood the algorithm intuitively and you know how it works you should be at least a bit disappointed. In particular, how does it not work?

Compare that to how a human might learn to play Pong. You show them the game and say something along the lines of "You're in control of a paddle and you can move it up and down, and your task is to bounce the ball past the other player controlled by AI", and you're set and ready to go. Notice some of the differences:

- In practical settings we usually communicate the task in some manner (e.g. English above), but in a standard RL problem you assume an arbitrary reward function that you have to discover through environment interactions. It can be argued that if a human went into game of Pong but without knowing anything about the reward function (indeed, especially if the reward function was some static but random function), the human would have a lot of difficulty learning what to do but Policy Gradients would be indifferent, and likely work much better. Similarly, if we took the frames and permuted the pixels randomly then humans would likely fail, but our Policy Gradient solution could not even tell the difference (if it's using a fully connected network as done here).
- A human brings in a huge amount of prior knowledge, such as intuitive physics (the ball bounces, it's unlikely to teleport, it's unlikely to suddenly stop, it maintains a constant velocity, etc.), and intuitive psychology (the AI opponent "wants" to win, is likely following an obvious strategy of moving towards the ball, etc.). You also understand the concept of being "in control" of a paddle, and that it responds to your UP/DOWN key commands. In contrast, our algorithms start from scratch which is simultaneously impressive (because it works) and depressing (because we lack concrete ideas for how not to).
- Policy Gradients are a *brute force* solution, where the correct actions are eventually discovered and internalized into a policy. Humans build a rich, abstract model and plan within it. In Pong, I can reason that the opponent is quite slow so it might be a good strategy to bounce the ball with high vertical velocity, which would cause the opponent to not catch it in time. However, it also feels as though we also eventually "internalize" good solutions into what feels more like a reactive muscle memory policy. For example if you're learning a new motor task (e.g. driving a car with stick shift?) you often feel yourself thinking a lot in the beginning but eventually the task becomes automatic and mindless.
- Policy Gradients have to actually experience a positive reward, and experience it very often in order to eventually and slowly shift the policy parameters towards repeating moves that give high rewards. With our abstract model, humans can figure out what is likely to give rewards without ever actually experiencing the rewarding or unrewarding transition. I don't have to actually experience crashing my car into a wall a few hundred times before I slowly start avoiding to do so.

<div class="imgcap">
<div style="display:inline-block">
	<img src="/assets/rl/montezuma.png" height="250">
</div>
<div style="display:inline-block; margin-left: 20px;">
	<img src="/assets/rl/frostbite.jpg" height="250">
</div>
<div class="thecap" style="text-align:justify;"><b>Left:</b> Montezuma's Revenge: a difficult game for our RL algorithms. The player must jump down, climb up, get the key, and open the door. A human understands that acquiring a key is useful. The computer samples billions of random moves and 99% of the time falls to its death or gets killed by the monster. In other words it's hard to "stumble into" the rewarding situation. <b>Right:</b> Another difficult game called Frostbite, where a human understands that things move, some things are good to touch, some things are bad to touch, and the goal is to build the igloo brick by brick. A good analysis of this game and a discussion of differences between the human and computer approach can be found in <a href="https://arxiv.org/abs/1604.00289">Building Machines That Learn and Think Like People</a>.</div>
</div>

I'd like to also emphasize the point that, conversely, there are many games where Policy Gradients would quite easily defeat a human. In particular, anything with frequent reward signals that requires precise play, fast reflexes, and not too much long-term planning would be ideal, as these short-term correlations between rewards and actions can be easily "noticed" by the approach, and the execution meticulously perfected by the policy. You can see hints of this already happening in our Pong agent: it develops a strategy where it waits for the ball and then rapidly dashes to catch it just at the edge, which launches it quickly and with high vertical velocity. The agent scores several points in a row repeating this strategy. There are many ATARI games where Deep Q Learning destroys human baseline performance in this fashion - e.g. Pinball, Breakout, etc.

In conclusion, once you understand the "trick" by which these algorithms work you can reason through their strengths and weaknesses. In particular, we are nowhere near humans in building abstract, rich representations of games that we can plan within and use for rapid learning. One day a computer will look at an array of pixels and notice a key, a door, and think to itself that it is probably a good idea to pick up the key and reach the door. For now there is nothing anywhere close to this, and trying to get there is an active area of research.

### Non-differentiable computation in Neural Networks

I'd like to mention one more interesting application of Policy Gradients unrelated to games: It allows us to design and train neural networks with components that perform (or interact with) non-differentiable computation. The idea was first introduced in [Williams 1992](http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf) and more recently popularized by [Recurrent Models of Visual Attention](http://arxiv.org/abs/1406.6247) under the name "hard attention", in the context of a model that processed an image with a sequence of low-resolution foveal glances (inspired by our own human eyes). In particular, at every iteration an RNN would receive a small piece of the image and sample a location to look at next. For example the RNN might look at position (5,30), receive a small piece of the image, then decide to look at (24, 50), etc. The problem with this idea is that there a piece of network that produces a distribution of where to look next and then samples from it. Unfortunately, this operation is non-differentiable because, intuitively, we don't know what would have happened if we sampled a different location. More generally, consider a neural network from some inputs to outputs:

<div class="imgcap">
<img src="/assets/rl/nondiff1.png" width="600">
</div>

Notice that most arrows (in blue) are differentiable as normal, but some of the representation transformations could optionally also include a non-differentiable sampling operation (in red). We can backprop through the blue arrows just fine, but the red arrow represents a dependency that we cannot backprop through.

Policy gradients to the rescue! We'll think about the part of the network that does the sampling as a small stochastic policy embedded in the wider network. Therefore, during training we will produce several samples (indicated by the branches below), and then we'll encourage samples that eventually led to good outcomes (in this case for example measured by the loss at the end). In other words we will train the parameters involved in the blue arrows with backprop as usual, but the parameters involved with the red arrow will now be updated independently of the backward pass using policy gradients, encouraging samples that led to low loss. This idea was also recently formalized nicely in [Gradient Estimation Using Stochastic Computation Graphs](http://arxiv.org/abs/1506.05254).

<div class="imgcap">
<img src="/assets/rl/nondiff2.png" width="600">
</div>

**Trainable Memory I/O**. You'll also find this idea in many other papers. For example, a [Neural Turing Machine](https://arxiv.org/abs/1410.5401) has a memory tape that they it read and write from. To do a write operation one would like to execute something like `m[i] = x`, where `i` and `x` are predicted by an RNN controller network. However, this operation is non-differentiable because there is no signal telling us what would have happened to the loss if we were to write to a different location `j != i`. Therefore, the NTM has to do *soft* read and write operations. It predicts an attention distribution `a` (with elements between 0 and 1 and summing to 1, and peaky around the index we'd like to write to), and then doing `for all i: m[i] = a[i]*x`. This is now differentiable, but we have to pay a heavy computational price because we have to touch every single memory cell just to write to one position. Imagine if every assignment in our computers had to touch the entire RAM!

However, we can use policy gradients to circumvent this problem (in theory), as done in [RL-NTM](http://arxiv.org/abs/1505.00521). We still predict an attention distribution `a`, but instead of doing the soft write we sample locations to write to: `i = sample(a); m[i] = x`. During training we would do this for a small batch of `i`, and in the end make whatever branch worked best more likely. The large computational advantage is that we now only have to read/write at a single location at test time. However, as pointed out in the paper this strategy is very difficult to get working because one must accidentally stumble by working algorithms through sampling. The current consensus is that PG works well only in settings where there are a few discrete choices so that one is not hopelessly sampling through huge search spaces.

However, with Policy Gradients and in cases where a lot of data/compute is available we can in principle dream big - for instance we can design neural networks that learn to interact with large, non-differentiable modules such as Latex compilers (e.g. if you'd like char-rnn to generate latex that compiles), or a SLAM system, or LQR solvers, or something. Or, for example, a superintelligence might want to learn to interact with the internet over TCP/IP (which is sadly non-differentiable) to access vital information needed to take over the world. That's a great example.

### Conclusions

We saw that Policy Gradients are a powerful, general algorithm and as an example we trained an ATARI Pong agent from raw pixels, from scratch, in [130 lines of Python](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5). More generally the same algorithm can be used to train agents for arbitrary games and one day hopefully on many valuable real-world control problems. I wanted to add a few more notes in closing:

**On advancing AI**. We saw that the algorithm works through a brute-force search where you jitter around randomly at first and must accidentally stumble into rewarding situations at least once, and ideally often and repeatedly before the policy distribution shifts its parameters to repeat the responsible actions. We also saw that humans approach these problems very differently, in what feels more like rapid abstract model building - something we have barely even scratched the surface of in research (although many people are trying). Since these abstract models are very difficult (if not impossible) to explicitly annotate, this is also why there is so much interest recently in (unsupervised) generative models and program induction.

**On use in complex robotics settings**. The algorithm does not scale naively to settings where huge amounts of exploration are difficult to obtain. For instance, in robotic settings one might have a single (or few) robots, interacting with the world in real time. This prohibits naive applications of the algorithm as I presented it in this post. One related line of work intended to mitigate this problem is [deterministic policy gradients](http://jmlr.org/proceedings/papers/v32/silver14.pdf) - instead of requiring samples from a stochastic policy and encouraging the ones that get higher scores, the approach uses a deterministic policy and gets the gradient information directly from a second network (called a *critic*) that models the score function. This approach can in principle be much more efficient in settings with very high-dimensional actions where sampling actions provides poor coverage, but so far seems empirically slightly finicky to get working. Another related approach is to scale up robotics, as we're starting to see with [Google's robot arm farm](http://googleresearch.blogspot.com/2016/03/deep-learning-for-robots-learning-from.html), or perhaps even [Tesla's Model S + Autopilot](http://qz.com/694520/tesla-has-780-million-miles-of-driving-data-and-adds-another-million-every-10-hours/).

There is also a line of work that tries to make the search process less hopeless by adding additional supervision. In many practical cases, for instance, one can obtain expert trajectories from a human. For example [AlphaGo](https://deepmind.com/alpha-go) first uses supervised learning to predict human moves from expert Go games and the resulting human mimicking policy is later finetuned with policy gradients on the "real" objective of winning the game. In some cases one might have fewer expert trajectories (e.g. from [robot teleoperation](https://www.youtube.com/watch?v=kZlg0QvKkQQ)) and there are techniques for taking advantage of this data under the umbrella of [apprenticeship learning](http://ai.stanford.edu/~pabbeel//thesis/thesis.pdf). Finally, if no supervised data is provided by humans it can also be in some cases computed with expensive optimization techniques, e.g. by [trajectory optimization](http://people.eecs.berkeley.edu/~igor.mordatch/policy/index.html) in a known dynamics model (such as \\(F=ma\\) in a physical simulator), or in cases where one learns an approximate local dynamics model (as seen in very promising framework of [Guided Policy Search](http://arxiv.org/abs/1504.00702)).

**On using PG in practice**. As a last note, I'd like to do something I wish I had done in my RNN blog post. I think I may have given the impression that RNNs are magic and automatically do arbitrary sequential problems. The truth is that getting these models to work can be tricky, requires care and expertise, and in many cases could also be an overkill, where simpler methods could get you 90%+ of the way there. The same goes for Policy Gradients. They are not automatic: You need a lot of samples, it trains forever, it is difficult to debug when it doesn't work. One should always try a BB gun before reaching for the Bazooka. In the case of Reinforcement Learning for example, one strong baseline that should always be tried first is the [cross-entropy method (CEM)](https://en.wikipedia.org/wiki/Cross-entropy_method), a simple stochastic hill-climbing "guess and check" approach inspired loosely by evolution. And if you insist on trying out Policy Gradients for your problem make sure you pay close attention to the *tricks* section in papers, start simple first, and use a variation of PG called [TRPO](https://arxiv.org/abs/1502.05477), which almost always works better and more consistently than vanilla PG [in practice](http://arxiv.org/abs/1604.06778). The core idea is to avoid parameter updates that change your policy too much, as enforced by a constraint on the KL divergence between the distributions predicted by the old and the new policy on a batch of data (instead of conjugate gradients the simplest instantiation of this idea could be implemented by doing a line search and checking the KL along the way).

And that's it! I hope I gave you a sense of where we are with Reinforcement Learning, what the challenges are, and if you're eager to help advance RL I invite you to do so within our [OpenAI Gym](https://gym.openai.com/) :) Until next time!
